import json
import glob
import os

from PIL import Image
from PIL import ImageDraw

import numpy as np
import torch
import torch.utils.data as data
import torchvision.transforms as transforms

from math import acos, sqrt, pi
import colorsys
import albumentations as A

import math

##################################################
# UTILS CODE FOR LOADING THE DATA
##################################################

def default_loader(path):
    return Image.open(path).convert('RGB')

def loadjson(path, objectsofinterest, img):
    """
    Loads the data from a json file.
    If there are no objects of interest, then load all the objects.
    """
    with open(path) as data_file:
        data = json.load(data_file)

    pointsBelief = []
    points_keypoints_2d = []
    pointsBoxes = []
    centroids = []

    translations = []
    rotations = []
    points = []

    for i_line in range(len(data['objects'])):
        info = data['objects'][i_line]

        if objectsofinterest is not None and objectsofinterest.lower() not in info['class'].lower():
            continue        

        # Parse bounding box
        box = info.get('bounding_box_minx_maxx_miny_maxy')
        if box:
            boxToAdd = [
                float(box[0]),  # minx -> top_left_x
                float(box[2]),  # miny -> top_left_y
                float(box[1]),  # maxx -> bottom_right_x
                float(box[3])   # maxy -> bottom_right_y
            ]
            boxpoint = [
                (boxToAdd[0], boxToAdd[1]),
                (boxToAdd[0], boxToAdd[3]),
                (boxToAdd[2], boxToAdd[1]),
                (boxToAdd[2], boxToAdd[3])
            ]
            pointsBoxes.append(boxpoint)

        # Parse 3D cuboid projected points
        points3d = []
        pointdata = info.get('projected_cuboid', [])
        for p in pointdata:
            points3d.append((p[0], p[1]))

        # Parse centroid (last point of projected_cuboid)
        if len(pointdata) == 9:
            centroid = pointdata[-1]  # Assume the centroid is the 9th point (added separately in the JSON loader)            
            centroids.append((centroid[0], centroid[1]))

        pointsBelief.append(points3d)
        points.append(points3d)

        # Parse translations
        location = info.get('location', [0, 0, 0])
        translations.append([location[0], location[1], location[2]])

        # Parse quaternion
        rot = info.get("quaternion_xyzw", [0, 0, 0, 1])
        rotations.append(rot)

    # Reorganize pointsBelief based on WANTED and ACTUAL mapping
    actual_to_wanted = [0, 4, 5, 1, 3, 7, 6, 2, 8]  # Map ACTUAL index to WANTED index

    # Reorder points inside each 3D cuboid
    reordered_pointsBelief = []
    for points3d in pointsBelief:
        reordered_points = [None] * len(actual_to_wanted)
        for actual_idx, wanted_idx in enumerate(actual_to_wanted):
            reordered_points[wanted_idx] = points3d[actual_idx]
        reordered_pointsBelief.append(reordered_points)

    pointsBelief = reordered_pointsBelief

    return {
        "pointsBelief": pointsBelief,
        "rotations": rotations,
        "translations": translations,
        "centroids": centroids,
        "points": points,
        "keypoints_2d": points_keypoints_2d,
    }

def loadimages(root):
    """
    Find all the images in the path and folders, return them in imgs.
    """
    imgs = []

    def add_json_files(path):
        for ext in ['png', 'jpg']:
            for imgpath in glob.glob(os.path.join(path, f"*.{ext}")):
                jsonpath = imgpath.replace(f".{ext}", ".json")
                if os.path.exists(imgpath) and os.path.exists(jsonpath):
                    relative_path = os.path.relpath(imgpath, root)
                    imgs.append((imgpath, relative_path, jsonpath))

    def explore(path):
        if not os.path.isdir(path):
            return
        folders = [os.path.join(path, o) for o in os.listdir(path)
                   if os.path.isdir(os.path.join(path, o))]
        if folders:
            for path_entry in folders:
                explore(path_entry)
        else:
            add_json_files(path)

    explore(root)

    return imgs

class MultipleVertexJson(data.Dataset):
    """
    Dataloader for the data generated by NDDS (https://github.com/NVIDIA/Dataset_Synthesizer).
    This is the same data as the data used in FAT.
    """
    def __init__(self, root, preprocessing_transform, transform=None,
            normal = None, test=False,
            loader = default_loader,
            objectsofinterest = "",
            save = False,
            data_size = None,
            sigma = 16
            ):
        ###################
        self.objectsofinterest = objectsofinterest
        self.loader = loader
        self.transform = transform
        self.root = root
        self.imgs = []
        self.test = test
        self.normal = normal
        self.save = save
        self.data_size = data_size
        self.sigma = sigma
        self.preprocessing_transform = preprocessing_transform

        def load_data(path):
            '''Recursively load the data.  This is useful to load all of the FAT dataset.'''
            imgs = loadimages(path)            

            # Check all the folders in path
            for name in os.listdir(str(path)):
                imgs += loadimages(os.path.join(path, name))
            return imgs

        self.imgs = load_data(root)

        # Shuffle the data, this is useful when we want to use a subset.
        np.random.shuffle(self.imgs)

    def __len__(self):
        # When limiting the number of data
        if not self.data_size is None:
            return int(self.data_size)

        return len(self.imgs)

    def __getitem__(self, index):
        """
        Depending on how the data loader is configured,
        this will return the debug info with the cuboid drawn on it,
        this happens when self.save is set to true.
        Otherwise, during training this function returns the
        belief maps and affinity fields and image as tensors.
        """
        path, name, txt = self.imgs[index]
        img = self.loader(path)

        loader = loadjson

        data = loader(txt, self.objectsofinterest,img)

        pointsBelief        =   data['pointsBelief']
        objects_centroid    =   data['centroids']
        points_keypoints    =   data['keypoints_2d']
        translations        =   data['translations']
        # from list to tensor
        translations      =   torch.tensor(translations).float().flatten()
        rotations           =   data['rotations'] 
        rotations = torch.tensor(rotations).float().flatten()

        # Note:  All point coordinates are in the image space, e.g., pixel value.
        # This is used when we do saving --- helpful for debugging
        #if self.test:
        if self.test:    # Use the save to debug the data
            draw = ImageDraw.Draw(img)
            
            # PIL drawing functions, here for sharing draw
            def DrawKeypoints(points):
                for key in points:
                    DrawDot(key,(12, 115, 170),7)

            def DrawLine(point1, point2, lineColor, lineWidth):
                if not point1 is None and not point2 is None:
                    draw.line([point1,point2],fill=lineColor,width=lineWidth)

            def DrawDot(point, pointColor, pointRadius):
                if not point is None:
                    xy = [point[0]-pointRadius, point[1]-pointRadius, point[0]+pointRadius, point[1]+pointRadius]
                    draw.ellipse(xy, fill=pointColor, outline=pointColor)

            def DrawCube(points, color = None):
                '''Draw cube with a thick solid line across the front top edge.'''
                lineWidthForDrawing = 2
                lineColor = (255, 215, 0)  # yellow-ish

                if not color is None:
                    lineColor = color

                # Define a list of RGB colors
                colors = [
                    (255, 0, 0),    # Red
                    (0, 255, 0),    # Green
                    (0, 0, 255),    # Blue
                    (255, 255, 0),  # Yellow
                    (0, 255, 255),  # Cyan
                    (255, 0, 255),  # Magenta
                    (255, 165, 0),  # Orange
                    (128, 0, 128),  # Purple
                    (50, 205, 50),  # Lime
                    (255, 192, 203),# Pink
                    (0, 128, 128),  # Teal
                    (165, 42, 42)   # Brown
                ]

                # draw front
                DrawLine(points[0], points[1], colors[0], lineWidthForDrawing)
                DrawLine(points[1], points[2], colors[1], lineWidthForDrawing)
                DrawLine(points[3], points[2], colors[2], lineWidthForDrawing)
                DrawLine(points[3], points[0], colors[3], lineWidthForDrawing)

                # draw back
                DrawLine(points[4], points[5], colors[4], lineWidthForDrawing)
                DrawLine(points[6], points[5], colors[5], lineWidthForDrawing)
                DrawLine(points[6], points[7], colors[6], lineWidthForDrawing)
                DrawLine(points[4], points[7], colors[7], lineWidthForDrawing)

                # draw sides
                DrawLine(points[0], points[4], colors[8], lineWidthForDrawing)
                DrawLine(points[7], points[3], colors[9], lineWidthForDrawing)
                DrawLine(points[5], points[1], colors[10], lineWidthForDrawing)
                DrawLine(points[2], points[6], colors[11], lineWidthForDrawing)

                # draw dots
                DrawDot(points[6], pointColor=(255,255,255), pointRadius = 3)
                DrawDot(points[7], pointColor=(0,0,0), pointRadius = 3)
                DrawDot(points[8], pointColor=(255,0,0), pointRadius = 3)

            # Draw all the found objects.
            for points_belief_objects in pointsBelief:
                DrawCube(points_belief_objects)
            for keypoint in points_keypoints:
                DrawKeypoints(keypoint)

            img = Image.fromarray(np.array(img))

        transform = self.preprocessing_transform
        if self.transform is not None:
            transform = A.Compose([self.transform, self.preprocessing_transform])

        keypoints = list(map(tuple, np.array(pointsBelief).reshape(-1, 2)))
        centroids = list(map(tuple, objects_centroid))
        
        transformed = transform(image=np.array(img), keypoints = keypoints, centroids=centroids)
        img = transformed['image']
        keypoints = np.array(transformed['keypoints']).reshape(np.array(pointsBelief).shape)
        centroids = transformed['centroids']

        beliefs = CreateBeliefMap(
            img,
            pointsBelief=keypoints,
            nbpoints = 9,
            sigma = self.sigma)

        beliefs = transforms.Resize((img.shape[1] // 8, img.shape[2] // 8))(beliefs)

        affinities = GenerateMapAffinity(img,8,keypoints,centroids,8)

        has_points_belief = len(pointsBelief) > 0

        if not has_points_belief:
            translations = torch.zeros(3)
            rotations = torch.zeros(4)

        return {
            'image': img,
            'translations': translations,
            'rotations': rotations,
            'has_points_belief': int(has_points_belief)
        }

"""
Some simple vector math functions to find the angle
between two points, used by affinity fields.
"""
def length(v):
    return sqrt(v[0]**2+v[1]**2)

def dot_product(v,w):
   return v[0]*w[0]+v[1]*w[1]

def normalize(v):
    norm=np.linalg.norm(v, ord=1)
    if norm==0:
        norm=np.finfo(v.dtype).eps
    return v/norm

def determinant(v,w):
   return v[0]*w[1]-v[1]*w[0]

def inner_angle(v,w):
   cosx=dot_product(v,w)/(length(v)*length(w))
   rad=acos(cosx) # in radians
   return rad*180/pi # returns degrees

def py_ang(A, B=(1,0)):
    inner=inner_angle(A,B)
    det = determinant(A,B)
    if det<0: #this is a property of the det. If the det < 0 then B is clockwise of A
        return inner
    else: # if the det > 0 then A is immediately clockwise of B
        return 360-inner

def GenerateMapAffinity(img,nb_vertex,pointsInterest,objects_centroid,scale):
    """
    Function to create the affinity maps,
    e.g., vector maps pointing toward the object center.

    Args:
        img: PIL image
        nb_vertex: (int) number of points
        pointsInterest: list of points
        objects_centroid: (x,y) centroids for the obects
        scale: (float) by how much you need to scale down the image
    return:
        return a list of tensors for each point except centroid point
    """

    # Apply the downscale right now, so the vectors are correct.
    img_affinity = Image.new('RGB', (int(img.shape[0]/scale),int(img.shape[1]/scale)), "black")

    affinities = []
    for i_points in range(nb_vertex):
        affinities.append(torch.zeros(2,int(img.shape[1]/scale),int(img.shape[2]/scale)))

    for i_pointsImage in range(len(pointsInterest)):
        pointsImage = pointsInterest[i_pointsImage]
        center = objects_centroid[i_pointsImage]
        for i_points in range(nb_vertex):
            point = pointsImage[i_points]
            affinity_pair, img_affinity = getAffinityCenter(int(img.shape[2]/scale),
                int(img.shape[1]/scale),
                tuple((np.array(pointsImage[i_points])/scale).tolist()),
                tuple((np.array(center)/scale).tolist()),
                img_affinity = img_affinity, radius=1)

            affinities[i_points] = (affinities[i_points] + affinity_pair)/2


            # Normalizing
            v = affinities[i_points].numpy()

            xvec = v[0]
            yvec = v[1]

            norms = np.sqrt(xvec * xvec + yvec * yvec)
            nonzero = norms > 0

            xvec[nonzero]/=norms[nonzero]
            yvec[nonzero]/=norms[nonzero]

            affinities[i_points] = torch.from_numpy(np.concatenate([[xvec],[yvec]]))
    affinities = torch.cat(affinities,0)

    return affinities

def getAffinityCenter(width, height, point, center, radius=7, img_affinity=None):
    """
    Function to create the affinity maps,
    e.g., vector maps pointing toward the object center.

    Args:
        width: image wight
        height: image height
        point: (x,y)
        center: (x,y)
        radius: pixel radius
        img_affinity: tensor to add to
    return:
        return a tensor
    """
    tensor = torch.zeros(2,height,width).float()

    # Create the canvas for the afinity output
    imgAffinity = Image.new("RGB", (width,height), "black")

    draw = ImageDraw.Draw(imgAffinity)
    r1 = radius
    p = point
    draw.ellipse((p[0]-r1,p[1]-r1,p[0]+r1,p[1]+r1),(255,255,255))

    del draw

    # Compute the array to add the afinity
    array = (np.array(imgAffinity)/255)[:,:,0]

    angle_vector = np.array(center) - np.array(point)
    angle_vector = normalize(angle_vector)
    affinity = np.concatenate([[array*angle_vector[0]],[array*angle_vector[1]]])

    # print (tensor)
    if not img_affinity is None:
        # Find the angle vector
        # print (angle_vector)
        if length(angle_vector) >0:
            angle=py_ang(angle_vector)
        else:
            angle = 0
        # print(angle)
        c = np.array(colorsys.hsv_to_rgb(angle/360,1,1)) * 255
        draw = ImageDraw.Draw(img_affinity)
        draw.ellipse((p[0]-r1,p[1]-r1,p[0]+r1,p[1]+r1),fill=(int(c[0]),int(c[1]),int(c[2])))
        del draw
    re = torch.from_numpy(affinity).float() + tensor
    return re, img_affinity

def CreateBeliefMap(img,pointsBelief,nbpoints,sigma=16):
    beliefsImg = []
    sigma = int(sigma)
    for numb_point in range(nbpoints):    
        array = np.zeros((img.shape[1], img.shape[2]))

        for point in pointsBelief:
            p = point[numb_point]
            w = int(sigma*2)
            if p[0]-w>=0 and p[0]+w<img.shape[2] and p[1]-w>=0 and p[1]+w<img.shape[1]:
                for x in range(int(p[0])-w, int(p[0])+w):
                    for y in range(int(p[1])-w, int(p[1])+w):
                        array[y,x] = np.exp(-(((x - p[0])**2 + (y - p[1])**2)/(2*(sigma**2))))

        beliefsImg.append(array)
    return torch.from_numpy(np.array(beliefsImg))

def make_grid(tensor, nrow=8, padding=2,
              normalize=False, img_range=None, scale_each=False, pad_value=0):
    """
    Make a grid of images.

    Args:
        tensor (Tensor or list): 4D mini-batch Tensor of shape (B x C x H x W)
            or a list of images all of the same size.
        nrow (int, optional): Number of images displayed in each row of the grid.
            The Final grid size is (B / nrow, nrow). Default is 8.
        padding (int, optional): amount of padding. Default is 2.
        normalize (bool, optional): If True, shift the image to the img_range (0, 1),
            by subtracting the minimum and dividing by the maximum pixel value.
        img_range (tuple, optional): tuple (min, max) where min and max are numbers,
            then these numbers are used to normalize the image. By default, min and max
            are computed from the tensor.
        scale_each (bool, optional): If True, scale each image in the batch of
            images separately rather than the (min, max) over all images.
        pad_value (float, optional): Value for the padded pixels.
    """
    if not (torch.is_tensor(tensor) or
            (isinstance(tensor, list) and all(torch.is_tensor(t) for t in tensor))):
        raise TypeError('tensor or list of tensors expected, got {}'.format(type(tensor)))

    # if list of tensors, convert to a 4D mini-batch Tensor
    if isinstance(tensor, list):
        tensor = torch.stack(tensor, dim=0)

    if tensor.dim() == 2:  # single image H x W
        tensor = tensor.view(1, tensor.size(0), tensor.size(1))
    if tensor.dim() == 3:  # single image
        if tensor.size(0) == 1:  # if single-channel, convert to 3-channel
            tensor = torch.cat((tensor, tensor, tensor), 0)
        tensor = tensor.view(1, tensor.size(0), tensor.size(1), tensor.size(2))

    if tensor.dim() == 4 and tensor.size(1) == 1:  # single-channel images
        tensor = torch.cat((tensor, tensor, tensor), 1)

    if normalize == True:
        tensor = tensor.clone()  # avoid modifying tensor in-place
        if img_range is not None:
            assert isinstance(img_range, tuple), \
                "img_range has to be a tuple (min, max) if specified. min and max are numbers"

        def norm_ip(img, min, max):
            img.clamp_(min=min, max=max)
            img.add_(-min).div_(max - min + 1e-5)

        def norm_range(t, img_range):
            if img_range is not None:
                norm_ip(t, img_range[0], img_range[1])
            else:
                norm_ip(t, float(t.min()), float(t.max()))

        if scale_each == True:
            for t in tensor:  # loop over mini-batch dimension
                norm_range(t, img_range)
        else:
            norm_range(tensor, img_range)

    if tensor.size(0) == 1:
        return tensor.squeeze()

    # make the mini-batch of images into a grid
    nmaps = tensor.size(0)
    xmaps = min(nrow, nmaps)
    ymaps = int(math.ceil(float(nmaps) / xmaps))
    height, width = int(tensor.size(2) + padding), int(tensor.size(3) + padding)
    grid = tensor.new(3, height * ymaps + padding, width * xmaps + padding).fill_(pad_value)
    k = 0
    for y in range(ymaps):
        for x in range(xmaps):
            if k >= nmaps:
                break
            grid.narrow(1, y * height + padding, height - padding)\
                .narrow(2, x * width + padding, width - padding)\
                .copy_(tensor[k])
            k = k + 1
    return grid


"""
def save_image(tensor, filename, nrow=4, padding=2, mean=None, std=None):
    \"""
    Saves a given Tensor into an image file.
    If given a mini-batch tensor, will save the tensor as a grid of images.
    \"""
    from PIL import Image

    tensor = tensor.cpu()
    grid = make_grid(tensor, nrow=nrow, padding=10, pad_value=1)
    
    if mean is not None and std is not None:
        # Unnormalize the grid
        mean = torch.tensor(mean).view(-1, 1, 1)  # Reshape to (C, 1, 1)
        std = torch.tensor(std).view(-1, 1, 1)    # Reshape to (C, 1, 1)
        grid = grid * std + mean  # Unnormalize

    ndarr = grid.mul(255).byte().transpose(0, 2).transpose(0, 1).numpy()
    grid = grid.clamp(0, 1)

    im = Image.fromarray(ndarr)
    im.save(filename)"""

def save_image(tensor, filename, nrow=4, padding=2, mean=None, std=None):
    """
    Saves a given Tensor into an image file.
    Optimized for performance.
    """
    from PIL import Image

    print("Saving image to", filename)

    # Move tensor to CPU and convert to NumPy directly
    tensor = tensor.cpu()

    # Normalize if mean and std are provided
    if mean is not None and std is not None:
        mean = torch.tensor(mean).view(-1, 1, 1)
        std = torch.tensor(std).view(-1, 1, 1)
        tensor = tensor * std + mean
    

    # Create the grid
    grid = make_grid(tensor, nrow=nrow, padding=padding, pad_value=1)

    # Convert to NumPy array and scale to [0, 255]
    ndarr = (grid.clamp(0, 1).mul(255).byte().permute(1, 2, 0).numpy())

    # Save the image
    im = Image.fromarray(ndarr)
    im.save(filename)